import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the authentication logs generated by generate_logs.py
# Assumes auth_logs.csv is in the same directory

# Read the CSV into a pandas DataFrame
df = pd.read_csv("auth_logs.csv")

# Convert the timestamp column to datetime and extract hour-of-day as a numeric feature
df["timestamp"] = pd.to_datetime(df["timestamp"])
df["hour"] = df["timestamp"].dt.hour

# We will train on the following columns
features = ["user", "status", "hour"]
X = df[features]

# Preprocess categorical features with one-hot encoding and pass through the numeric hour feature
preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), ["user", "status"]),
        ("num", "passthrough", ["hour"]),
    ]
)

# Create an Isolation Forest model. The contamination parameter specifies the expected fraction of anomalies.
iso_forest = IsolationForest(contamination=0.05, random_state=42)

# Build a pipeline that first preprocesses the data, then fits the model
pipeline = Pipeline([
    ("preprocess", preprocess),
    ("model", iso_forest),
])

# Fit the model
pipeline.fit(X)

# Use the trained model to compute anomaly scores
# Higher positive scores indicate more normal instances; we negate them so higher means more anomalous
df["anomaly_score"] = -pipeline.decision_function(X)

# Sort by anomaly score descending to get the most anomalous events first
df_sorted = df.sort_values("anomaly_score", ascending=False)

# Save the top anomalies to a CSV for inspection
df_sorted.head(20).to_csv("anomalies.csv", index=False)
print("Top anomalies saved to anomalies.csv")
